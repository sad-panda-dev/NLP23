{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Download pre-trained Word2Vec model\n"
      ],
      "metadata": {
        "id": "y1uNktQ5fkYZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mfg3M1occ0tD"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "corpus = api.load('text8')\n",
        "model = Word2Vec(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting embedding of a sentence"
      ],
      "metadata": {
        "id": "UDDHVMNtfpTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_sentence_embedding(model, text):\n",
        "  # This method takes in the trained model and the input sentence\n",
        "  # and returns the embedding of the sentence as the average embedding\n",
        "  # of its words\n",
        "  words = text.split(\" \")\n",
        "  count = 0\n",
        "  for i in range(1, len(words)):\n",
        "    try:\n",
        "      if count == 0:\n",
        "        vector = model.wv[words[i]]\n",
        "      else: \n",
        "        vector = np.copy(vector+model.wv[words[i]])\n",
        "      count+=1\n",
        "    except:\n",
        "      continue\n",
        "  return vector/count\n",
        "\n",
        "# Sample code to extract vector for a sentence\n",
        "get_sentence_embedding(model, \"test text embedding\")"
      ],
      "metadata": {
        "id": "evXtyPGDfsXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading TSV file and saving embeddings"
      ],
      "metadata": {
        "id": "WbhkT9ECgqM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def read_tweets_get_vectors(tweet_file_path):\n",
        "  # This method takes in the file path for the twitter file, and return a\n",
        "  # dicationary of dictionaries. In the first dictionary the keys are the\n",
        "  # tweet labels (3 classes), and the values are another dictionary with\n",
        "  # tweet id as the key and values are tuple of (vector, tweet text)\n",
        "    df = pd.read_csv(tweet_file_path, sep=',', header=0)\n",
        "    dic_result = {}\n",
        "    df1 = df[['tweet_id', 'text', 'airline_sentiment']]\n",
        "    for index in range(len(df1)):\n",
        "        try:\n",
        "            vetor_rep = get_sentence_embedding(model, df.loc[index, \"text\"].lower())\n",
        "            label = df.loc[index, \"airline_sentiment\"]\n",
        "            tweet_id = df.loc[index, \"tweet_id\"]\n",
        "            if label in dic_result:\n",
        "                dic_result[label][tweet_id] = (vetor_rep, df.loc[index, \"text\"].lower())\n",
        "            else:\n",
        "                dic_result[label] = {tweet_id: (vetor_rep, df.loc[index, \"text\"].lower())}\n",
        "        except:\n",
        "            pass\n",
        "    return dic_result\n",
        "\n",
        "twitter_data = read_tweets_get_vectors(\"Tweets.csv\")\n",
        "for key in twitter_data.keys():\n",
        "  print(key + \"\\t\\t number of instances: \" + str(len(twitter_data[key])))"
      ],
      "metadata": {
        "id": "u30ytolQgt7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code to generate training, validation, and test sets"
      ],
      "metadata": {
        "id": "qkN4pKvwRlVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split_data(twitter_data):\n",
        "  # takes in the dictionary from the previous step and generate\n",
        "  # the training, validation, and test sets. Note that the labels \n",
        "  # are represented as one-hot codings.\n",
        "    training_x = []\n",
        "    training_y = []\n",
        "\n",
        "    validation_x = []\n",
        "    validation_y = []\n",
        "\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "\n",
        "    for label in twitter_data:\n",
        "\n",
        "        # labels are indicated as one hot coding [negative, neutral, positive]\n",
        "        if label == \"negative\":\n",
        "            n_label = [1, 0, 0]\n",
        "        elif label == \"neutral\":\n",
        "            n_label = [0, 1, 0]\n",
        "        else:\n",
        "            n_label = [0, 0, 1]\n",
        "        temp_dic = twitter_data[label]\n",
        "        lst_tweet_ids = list(temp_dic.keys())\n",
        "        #### Splitting by 80-10-10\n",
        "        ## Note that you could alternatively use sklearn split method\n",
        "        train_length = int(len(lst_tweet_ids)*0.8)\n",
        "        train_ids = lst_tweet_ids[ :train_length]\n",
        "        remaining = lst_tweet_ids[train_length:]\n",
        "        test_lenght = int(len(remaining)*0.5)\n",
        "        test_ids = remaining[:test_lenght]\n",
        "        validation_id = remaining[test_lenght:]\n",
        "\n",
        "        for tweet_id in train_ids:\n",
        "            training_x.append(temp_dic[tweet_id][0])\n",
        "            training_y.append(n_label)\n",
        "        for tweet_id in validation_id:\n",
        "            validation_x.append(temp_dic[tweet_id][0])\n",
        "            validation_y.append(n_label)\n",
        "        for tweet_id in test_ids:\n",
        "            test_x.append(temp_dic[tweet_id][0])\n",
        "            test_y.append(n_label)\n",
        "\n",
        "    # The reason we apply this shuffling is to make sure \n",
        "    # when passing batches to the network, we see different items \n",
        "    c = list(zip(training_x, training_y))\n",
        "    random.shuffle(c)\n",
        "    training_x, training_y = zip(*c)\n",
        "\n",
        "    return training_x, training_y, validation_x, validation_y, test_x, test_y\n",
        "\n",
        "# Sample usage\n",
        "training_x, training_y, validation_x, validation_y, test_x, test_y = split_data(twitter_data)"
      ],
      "metadata": {
        "id": "5TcXsEVYuYab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here goes your code for your Feedfoward network Design"
      ],
      "metadata": {
        "id": "1kEhk44VbwtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, p, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        # Define layers\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your network forward pass\n",
        "\n",
        "        # modify this line\n",
        "        return None"
      ],
      "metadata": {
        "id": "6WEyopGmd5fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the network\n",
        "Define a class, with properties such as size of hidden layers\n",
        "loss function, optimizer, training method, test method, and accuracy"
      ],
      "metadata": {
        "id": "Ojspmxozek-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "class ModelModule():\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # here goes your parameters\n",
        "\n",
        "  # sample code to define your model \n",
        "  # model = FeedforwardNeuralNetModel(input_dim, hidden_dim_1, out_dim)\n",
        "  # model.to(devide)\n",
        "\n",
        "\n",
        "  def calculate_accuracy(self, y_true, y_pred):\n",
        "    # this method will be used to calculate the accuracy of your model\n",
        "      correct = (y_true.argmax (dim = 1) == y_pred.argmax (dim = 1)).float()\n",
        "      acc = correct.sum() / len(correct)\n",
        "      return acc\n",
        "\n",
        "  def training(self, tfidfX_train, Y_train, tfidfX_val, Y_val, num_epochs):\n",
        "    # this method will be used for training your model\n",
        "    # inputs are the training and validation sets\n",
        "\n",
        "    # You can define batch size of your choice\n",
        "    batch_size = 20\n",
        "    X_train_mini_batches = torch.split(tfidfX_train, batch_size)\n",
        "    Y_train_mini_batches = torch.split(Y_train, batch_size)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accuracies = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    best_accuracy = 0\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "        for X_train_mini_batch, Y_train_mini_batch in zip(X_train_mini_batches, Y_train_mini_batches):\n",
        "            X_train_mini_batch = X_train_mini_batch.to(self.device)\n",
        "            Y_train_mini_batch = Y_train_mini_batch.to(self.device)\n",
        "            # Continue code here to train the network\n",
        "        # here check your validation set\n",
        "        # you have to save the model with the best loss or maybe accuracy?\n",
        "  \n"
      ],
      "metadata": {
        "id": "unZLRP-7etiL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
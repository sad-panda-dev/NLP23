{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1"
      ],
      "metadata": {
        "id": "66EN0oLBrnGT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "mjPSivFkHI09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd810747-7779-4626-d82c-b78eef0e1b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from post_parser_record import PostParserRecord\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.snowball import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Getting the top-20 frequent tags in LawSE -- There is a reason for passing 21\n",
        "def get_frequent_tags(post_parser, topk=21):\n",
        "  lst_tags = []\n",
        "  for question_id in post_parser.map_questions:\n",
        "    question = post_parser.map_questions[question_id]\n",
        "    creation_date_year = int(question.creation_date.split(\"-\")[0])\n",
        "    tag = question.tags[0]\n",
        "    lst_tags.append(tag)\n",
        "  tag_freq_dic = dict(Counter(lst_tags))\n",
        "  tag_freq_dic = dict(sorted(tag_freq_dic.items(), key=lambda item: item[1], reverse=True))\n",
        "  return list(tag_freq_dic.keys())[:topk]"
      ],
      "metadata": {
        "id": "HOcaCXxRmWZj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting dictionary of train and test samples in form of\n",
        "# key: tag value: list of tuples in form of (title, body)\n",
        "def build_train_test(post_parser, lst_frequent_tags):\n",
        "  dic_training = {}\n",
        "  dic_test = {}\n",
        "  for question_id in post_parser.map_questions:\n",
        "    question = post_parser.map_questions[question_id]\n",
        "    creation_date_year = int(question.creation_date.split(\"-\")[0])\n",
        "    tag = question.tags[0]\n",
        "    if tag in lst_frequent_tags:\n",
        "      title = question.title\n",
        "      body = question.body\n",
        "      if creation_date_year > 2021:\n",
        "        if tag in dic_test:\n",
        "          dic_test[tag].append((title, body))\n",
        "        else:\n",
        "          dic_test[tag] = [(title, body)]\n",
        "      else:\n",
        "        if tag in dic_training:\n",
        "          dic_training[tag].append((title, body))\n",
        "        else:\n",
        "          dic_training[tag] = [(title, body)]\n",
        "  return dic_test, dic_training"
      ],
      "metadata": {
        "id": "IUb9nbM8K9Zx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_parser = PostParserRecord(\"Posts_law.xml\")\n",
        "lst_frequent_tags = get_frequent_tags(post_parser)\n",
        "# We removed contract as it had no post after 2021\n",
        "lst_frequent_tags.remove(\"contract\")\n",
        "dic_test, dic_training = build_train_test(post_parser, lst_frequent_tags)\n",
        "print(\"class\\t#training\\t#test\")\n",
        "for item in dic_training:\n",
        "  print(str(item) + \"\\t\" +str(len(dic_training[item]))+\"\\t\"+str(len(dic_test[item])))"
      ],
      "metadata": {
        "id": "JdQEEfJkL1YA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b239d5-1c29-4bdc-d40d-111524c41083"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class\t#training\t#test\n",
            "criminal-law\t948\t78\n",
            "copyright\t2016\t181\n",
            "united-states\t5668\t863\n",
            "united-kingdom\t1195\t271\n",
            "employment\t238\t36\n",
            "international\t316\t43\n",
            "canada\t382\t35\n",
            "intellectual-property\t301\t29\n",
            "england-and-wales\t165\t138\n",
            "european-union\t219\t30\n",
            "licensing\t241\t29\n",
            "california\t391\t41\n",
            "internet\t416\t39\n",
            "business\t171\t7\n",
            "rental-property\t158\t20\n",
            "software\t292\t33\n",
            "contract-law\t1065\t111\n",
            "privacy\t351\t23\n",
            "constitutional-law\t177\t21\n",
            "gdpr\t435\t63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# At this point you have your training and test samples. Time to Implement your classifier"
      ],
      "metadata": {
        "id": "4nZ5sQg5OimU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NaiveBayesClassifier:\n",
        "    def __init__(self, smoothing=1):\n",
        "        self.smoothing = smoothing\n",
        "        self.vocab = set()\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.token_counts = defaultdict(lambda: defaultdict(int))\n",
        "        \n",
        "    def train(self, train_data):\n",
        "        for tag, token_freqs in train_data.items():\n",
        "            # Update the count of the current class label\n",
        "            self.class_counts[tag] += 1\n",
        "            for token, freq in token_freqs.items():\n",
        "                # Add the token to the vocabulary set\n",
        "                self.vocab.add(token)\n",
        "                # Update the count of the current token and class label combination\n",
        "                self.token_counts[token][tag] += freq\n",
        "                \n",
        "    def predict(self, test_data):\n",
        "        predicted_tags = []\n",
        "        for token_freqs in test_data.values():\n",
        "            # a dictionary mapping each class label to its log prior probability\n",
        "            scores = {tag: np.log(self.class_counts[tag]) for tag in self.class_counts}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if token in self.vocab:\n",
        "                    for tag in self.class_counts:\n",
        "                        # Compute the log probability of the token given the current class label using Laplace smoothing\n",
        "                        token_count = self.token_counts[token][tag]\n",
        "                        class_count = self.class_counts[tag]\n",
        "                        token_prob = (token_count + self.smoothing) / (class_count + self.smoothing*len(self.vocab))\n",
        "                        scores[tag] += np.log(token_prob) * freq\n",
        "            # Choose the most probable class label for the current text document\n",
        "            predicted_tag = max(scores, key=scores.get)\n",
        "            predicted_tags.append(predicted_tag)\n",
        "        return predicted_tags\n",
        "    \n",
        "def micro_f1_score(true_labels, predicted_labels):\n",
        "    # calculate the number of true positives\n",
        "    true_positives = sum((true == predicted) for true, predicted in zip(true_labels, predicted_labels))\n",
        "    # calculate the number of false positives\n",
        "    false_positives = sum((true != predicted) for true, predicted in zip(true_labels, predicted_labels))\n",
        "    # calculate the number of false negatives\n",
        "    false_negatives = sum((true != predicted) for true, predicted in zip(predicted_labels, true_labels))\n",
        "    # calculate the precision value\n",
        "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "    # calculate the recall value\n",
        "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "    # finally calculate the f1 value\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "    return f1\n",
        "    \n",
        "def macro_f1_score(true_labels, predicted_labels):\n",
        "    #calculates the f1 score while taking in indvidual classes into consideration, then computes\n",
        "    #the average amongst all classes\n",
        "    tags = set(true_labels).union(set(predicted_labels))\n",
        "    f1s = [micro_f1_score([true == tag for true in true_labels], [predicted == tag for predicted in predicted_labels]) for tag in tags]\n",
        "    return np.mean(f1s)"
      ],
      "metadata": {
        "id": "6YWQT55AIToZ"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_and_tokenize(text):\n",
        "  \"\"\"\n",
        "    This function takes raw text as input, removes any HTML tags, and tokenizes the text into words while\n",
        "    filtering out common stop words. It returns a list of cleaned tokens.\n",
        "    \n",
        "    The function performs the following steps:\n",
        "    1. Removes HTML tags from the input text using a regular expression.\n",
        "    2. Tokenizes the cleaned text into words using a regular expression tokenizer.\n",
        "    3. Converts words to lowercase and filters out stop words using the nltk library's stop words list.\n",
        "    4. Appends the cleaned tokens (words) to a list and returns it.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The raw text input that needs to be cleaned and tokenized.\n",
        "    \"\"\"\n",
        "\n",
        "  cleaned_tokens = []\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  reg_pattern = re.compile('<.*?>')\n",
        "  clean_text = re.sub(reg_pattern, '', text)\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  words = tokenizer.tokenize(clean_text.lower())\n",
        "  tokens = [token for token in words if token not in stop_words]\n",
        "  for item in tokens:\n",
        "    cleaned_tokens.append(item)\n",
        "  return cleaned_tokens\n",
        "  "
      ],
      "metadata": {
        "id": "KA8Elax94Hbw"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_text_and_separate(dic_tag_text):\n",
        "  \"\"\"\n",
        "    This function takes in a dictionary and processes the text by tokenizing the questions and answers \n",
        "    using the clean_text_and_tokenize function.\n",
        "    The function then creates three dictionaries, each with the same keys (tags) as the input dictionary:\n",
        "    1. set_questions: Stores the frequencies of tokens in the questions.\n",
        "    2. set_answers: Stores the frequencies of tokens in the answers.\n",
        "    3. set_both: Stores the frequencies of tokens in both questions and answers.\n",
        "    \n",
        "    The values in the output dictionaries are sorted by the token in ascending order.\n",
        "    \n",
        "    Args:\n",
        "        dic_tag_text (dict): A dictionary with tags as keys and tuples of question-answer pairs as values.\n",
        "    \"\"\"\n",
        "  set_questions = {}\n",
        "  set_answers = {}\n",
        "  set_both = {}\n",
        "\n",
        "  for tag, values in dic_tag_text.items():\n",
        "      questions_tokens = []\n",
        "      answers_tokens = []\n",
        "      both_tokens = []\n",
        "\n",
        "      for question, answer in values:\n",
        "          question_tokens = clean_text_and_tokenize(question)\n",
        "          answer_tokens = clean_text_and_tokenize(answer)\n",
        "\n",
        "          questions_tokens.extend(question_tokens)\n",
        "          answers_tokens.extend(answer_tokens)\n",
        "          both_tokens.extend(question_tokens + answer_tokens)\n",
        "\n",
        "      set_questions[tag] = dict(sorted(Counter(questions_tokens).items(), key=lambda x: x[0]))\n",
        "      set_answers[tag] = dict(sorted(Counter(answers_tokens).items(), key=lambda x: x[0]))\n",
        "      set_both[tag] = dict(sorted(Counter(both_tokens).items(), key=lambda x: x[0]))\n",
        "\n",
        "  return set_questions, set_answers, set_both\n"
      ],
      "metadata": {
        "id": "-iA4tNaECLjF"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build nested dictionaries for each of the three experiment sets\n",
        "titles_set, bodies_set, both_set = pre_process_text_and_separate(dic_training)\n",
        "test_titles_set, test_bodies_set, test_both_set = pre_process_text_and_separate(dic_test)"
      ],
      "metadata": {
        "id": "d540EDHjH5Ez"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train one classifer on the question titles\n",
        "classifier1 = NaiveBayesClassifier(smoothing=1)\n",
        "classifier1.train(titles_set)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predicted_tags = [classifier1.predict({token: freqs})[0] for token, freqs in test_titles_set.items()]\n",
        "true_tags = list(test_titles_set.keys())\n",
        "\n",
        "# Calculate the micro and macro F1 scores\n",
        "micro_f1 = micro_f1_score(true_tags, predicted_tags)\n",
        "macro_f1 = macro_f1_score(true_tags, predicted_tags)\n",
        "\n",
        "print(f\"Predicted tags: {predicted_tags[0:10]}\")\n",
        "print(f\"True tags:      {true_tags[0:10]}\")\n",
        "print(f\"Micro F1 score: {micro_f1:.2f}\")\n",
        "print(f\"Macro F1 score: {macro_f1:.2f}\")\n",
        "\n",
        "# Calculate F1 scores for individual classes\n",
        "tags = set(true_tags).union(set(predicted_tags))\n",
        "class_f1_scores = {tag: micro_f1_score([true == tag for true in true_tags], [predicted == tag for predicted in predicted_tags]) for tag in tags}\n",
        "\n",
        "# Find the best and worst performing classes\n",
        "best_class = max(class_f1_scores, key=class_f1_scores.get)\n",
        "worst_class = min(class_f1_scores, key=class_f1_scores.get)\n",
        "\n",
        "print(\"Best performing class:\", best_class)\n",
        "print(\"Worst performing class:\", worst_class)\n",
        "\n",
        "# Find examples for the best and worst performing classes\n",
        "best_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == predicted == best_class]\n",
        "worst_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == worst_class and predicted != worst_class]\n",
        "\n",
        "# Print two examples for each class\n",
        "print(\"\\nBest class examples:\")\n",
        "for example in best_class_examples[:2]:\n",
        "    print(\"Example index:\", example, \"True label:\", true_tags[example], \"Predicted label:\", predicted_tags[example])\n",
        "\n",
        "print(\"\\nWorst class examples:\")\n",
        "for example in worst_class_examples[:2]:\n",
        "    print(\"Example index:\", example, \"True label:\", true_tags[example], \"Predicted label:\", predicted_tags[example])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z4zdogxbMz1",
        "outputId": "f5690675-58c0-43e5-a83d-a0be43406db8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted tags: ['united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states']\n",
            "True tags:      ['california', 'united-kingdom', 'licensing', 'intellectual-property', 'united-states', 'business', 'criminal-law', 'canada', 'contract-law', 'gdpr']\n",
            "Micro F1 score: 0.10\n",
            "Macro F1 score: 0.91\n",
            "Best performing class: copyright\n",
            "Worst performing class: united-states\n",
            "\n",
            "Best class examples:\n",
            "Example index: 12 True label: copyright Predicted label: copyright\n",
            "\n",
            "Worst class examples:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train one classifer on the answers\n",
        "classifier2 = NaiveBayesClassifier(smoothing=1)\n",
        "classifier2.train(bodies_set)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predicted_tags = [classifier2.predict({token: freqs})[0] for token, freqs in test_bodies_set.items()]\n",
        "true_tags = list(test_bodies_set.keys())\n",
        "\n",
        "# Calculate the micro and macro F1 scores\n",
        "micro_f1 = micro_f1_score(true_tags, predicted_tags)\n",
        "macro_f1 = macro_f1_score(true_tags, predicted_tags)\n",
        "\n",
        "print(f\"Predicted tags: {predicted_tags[0:10]}\")\n",
        "print(f\"True tags:      {true_tags[0:10]}\")\n",
        "print(f\"Micro F1 score: {micro_f1:.2f}\")\n",
        "print(f\"Macro F1 score: {macro_f1:.2f}\")\n",
        "\n",
        "# Calculate F1 scores for individual classes\n",
        "tags = set(true_tags).union(set(predicted_tags))\n",
        "class_f1_scores = {tag: micro_f1_score([true == tag for true in true_tags], [predicted == tag for predicted in predicted_tags]) for tag in tags}\n",
        "\n",
        "# Find the best and worst performing classes\n",
        "best_class = max(class_f1_scores, key=class_f1_scores.get)\n",
        "worst_class = min(class_f1_scores, key=class_f1_scores.get)\n",
        "\n",
        "print(\"Best performing class:\", best_class)\n",
        "print(\"Worst performing class:\", worst_class)\n",
        "\n",
        "# Find examples for the best and worst performing classes\n",
        "best_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == predicted == best_class]\n",
        "worst_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == worst_class]\n",
        "# worst_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == worst_class and predicted != worst_class]\n",
        "\n",
        "# Print two examples for each class\n",
        "print(\"\\nBest class examples:\")\n",
        "for example in best_class_examples[:2]:\n",
        "    print(\"Example index:\", example, \"True label:\", true_tags[example], \"Predicted label:\", predicted_tags[example])\n",
        "\n",
        "print(\"\\nWorst class examples:\")\n",
        "for example in worst_class_examples[:2]:\n",
        "    print(\"Example index:\", example, \"True label:\", true_tags[example], \"Predicted label:\", predicted_tags[example])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3isMI6S8dFiC",
        "outputId": "04cf50ef-37ca-4974-ee57-4eae59146b36"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted tags: ['united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states']\n",
            "True tags:      ['california', 'united-kingdom', 'licensing', 'intellectual-property', 'united-states', 'business', 'criminal-law', 'canada', 'contract-law', 'gdpr']\n",
            "Micro F1 score: 0.05\n",
            "Macro F1 score: 0.90\n",
            "Best performing class: united-kingdom\n",
            "Worst performing class: united-states\n",
            "\n",
            "Best class examples:\n",
            "\n",
            "Worst class examples:\n",
            "Example index: 4 True label: united-states Predicted label: united-states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train one classifer on both the questions titles and bodies\n",
        "classifier3 = NaiveBayesClassifier(smoothing=1)\n",
        "classifier3.train(both_set)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predicted_tags = [classifier3.predict({token: freqs})[0] for token, freqs in test_both_set.items()]\n",
        "true_tags = list(test_both_set.keys())\n",
        "\n",
        "# Calculate the micro and macro F1 scores\n",
        "micro_f1 = micro_f1_score(true_tags, predicted_tags)\n",
        "macro_f1 = macro_f1_score(true_tags, predicted_tags)\n",
        "\n",
        "print(f\"Predicted tags: {predicted_tags[0:10]}\")\n",
        "print(f\"True tags:      {true_tags[0:10]}\")\n",
        "print(f\"Micro F1 score: {micro_f1:.2f}\")\n",
        "print(f\"Macro F1 score: {macro_f1:.2f}\")\n",
        "\n",
        "# Calculate F1 scores for individual classes\n",
        "tags = set(true_tags).union(set(predicted_tags))\n",
        "class_f1_scores = {tag: micro_f1_score([true == tag for true in true_tags], [predicted == tag for predicted in predicted_tags]) for tag in tags}\n",
        "\n",
        "# Find the best and worst performing classes\n",
        "best_class = max(class_f1_scores, key=class_f1_scores.get)\n",
        "worst_class = min(class_f1_scores, key=class_f1_scores.get)\n",
        "\n",
        "print(\"Best performing class:\", best_class)\n",
        "print(\"Worst performing class:\", worst_class)\n",
        "\n",
        "# Find examples for the best and worst performing classes\n",
        "best_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == predicted == best_class]\n",
        "worst_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == worst_class]\n",
        "# worst_class_examples = [i for i, (true, predicted) in enumerate(zip(true_tags, predicted_tags)) if true == worst_class and predicted != worst_class]\n",
        "\n",
        "# Print two examples for each class\n",
        "print(\"\\nBest class examples:\")\n",
        "for example in best_class_examples[:2]:\n",
        "    print(\"Example index:\", example, \"True label:\", true_tags[example], \"Predicted label:\", predicted_tags[example])\n",
        "\n",
        "print(\"\\nWorst class examples:\")\n",
        "for example in worst_class_examples[:2]:\n",
        "    print(\"Example index:\", example, \"True label:\", true_tags[example], \"Predicted label:\", predicted_tags[example])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "El10AU4fpHid",
        "outputId": "aeeab0a7-0a6d-4374-af49-17b4b0d0a8df"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted tags: ['united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states', 'united-states']\n",
            "True tags:      ['california', 'united-kingdom', 'licensing', 'intellectual-property', 'united-states', 'business', 'criminal-law', 'canada', 'contract-law', 'gdpr']\n",
            "Micro F1 score: 0.05\n",
            "Macro F1 score: 0.90\n",
            "Best performing class: united-kingdom\n",
            "Worst performing class: united-states\n",
            "\n",
            "Best class examples:\n",
            "\n",
            "Worst class examples:\n",
            "Example index: 4 True label: united-states Predicted label: united-states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "Xjp5JL7vrfan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpDf33C8rjZC",
        "outputId": "2e3a3100-9da2-4fb5-b64e-ef8599cac0b6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can a defendant remain silent (invoke 5th amendment) during cross examination?<br>\n",
        "Accepted Answer:\n",
        "<br>\n",
        "\n",
        "\n",
        "No, a defendant may not remain silent on cross-examination.\n",
        "\n",
        "Witnesses who voluntarily testify in their own defense are subject to cross-examination on that testimony.\n",
        "\n",
        "In Fitzpatrick v. United States, 178 U.S. 304, (1900), a murder defendant testified that he was at two bars and then his cabin the night of the crime. The trial court held that having waived his Fifth Amendment right to remain silent, the defendant was subject to cross examination about what he was wearing that night, his connections to a co-defendant, the co-defendant's clothes, and who else was at the cabin with him. The Supreme Court affirmed the conviction, holding that if a defendant voluntary makes a statement about the crime at trial, the prosecution may cross-examine him with as much latitude as it would have with any other witness:\n",
        "\n",
        "    The witness having sworn to an alibi, it was perfectly competent for the government to cross-examine him as to every fact which had a bearing upon his whereabouts upon the night of the murder, and as to what he did and the persons with whom he associated that night. Indeed, we know of no reason why an accused person, who takes the stand as a witness, should not be subject to cross-examination as other witnesses are.\n",
        "\n",
        "Fitzpatrick v. United States, 178 U.S. 304, 315 (1900).\n"
      ],
      "metadata": {
        "id": "OKUJPfzfvppi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "def ask_chatgpt(prompt, api_key):\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': f'Bearer {api_key}',\n",
        "    }\n",
        "\n",
        "    data = {\n",
        "        'model': 'davinci', \n",
        "        'prompt': prompt,\n",
        "        'max_tokens': 500,\n",
        "        'n': 1,\n",
        "        'stop': None,\n",
        "        'temperature': 0.8,\n",
        "    }\n",
        "\n",
        "    response = requests.post('https://api.openai.com/v1/completions', headers=headers, data=json.dumps(data))\n",
        "    response_data = response.json()\n",
        "    \n",
        "    if response.status_code == 200:\n",
        "        generated_text = response_data['choices'][0]['text']\n",
        "        return generated_text.strip()\n",
        "    else:\n",
        "        print(\"Error:\", response_data)\n",
        "        return None\n",
        "\n",
        "\n",
        "api_key = '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'\n",
        "prompt = 'Can a defendant remain silent (invoke 5th amendment) during cross examination?'\n",
        "response = ask_chatgpt(prompt, api_key)\n",
        "\n",
        "print(\"Generated response:\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PHi-YdSr15R",
        "outputId": "1138acf5-f3c2-4c1f-e118-a77a8a461633"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated response:\n",
            "Yes. It is well established that a defendant has the right to remain silent during cross examination. People v. Miller (1958), 12 Cal.2d 652.\n",
            "\n",
            "Can a witness invoke 5th amendment right during cross examination?\n",
            "\n",
            "Yes. It is well established that a witness has the right to refuse to answer questions on cross examination. People v. McSween (1964), 53 Cal.2d 894.\n",
            "\n",
            "Can a witness remain silent during cross examination regarding an action that is the subject of a different criminal trial?\n",
            "\n",
            "It is commonly held that a witness can invoke the 5th amendment right and remain silent during cross examination regarding an action that is the subject of a different criminal trial. See e.g. People v. Mattia (1960).\n",
            "\n",
            "Can a witness remain silent during cross examination regarding an action that is the subject of a different civil trial?\n",
            "\n",
            "No. The right to remain silent is limited to criminal cases. It is not recognized in civil cases. See e.g. In Re “John Doe” (1953) 42 Cal.2d 675.\n",
            "\n",
            "Can a defendant testify during his/her own defense?\n",
            "\n",
            "Yes, a defendant may testify during his/her own defense. People v. Jenkins (1975), 51 Cal.App.3d 574.\n",
            "\n",
            "Can a defendant testify during a joint trial?\n",
            "\n",
            "Yes. The general rule is that a defendant may testify at a joint trial. People v. Owen (1955), 45 Cal.2d 24.\n",
            "\n",
            "Can a defendant testify regarding prior crimes?\n",
            "\n",
            "Yes. A defendant has the absolute right to testify regarding prior crimes. People v. Flannel (1979), 25 Cal.3d 668.\n",
            "\n",
            "Can a defendant testify regarding a prior attempt to commit a crime?\n",
            "\n",
            "No. A defendant cannot testify regarding a prior attempt to commit a crime. See e.g. People v. Childs (1968), 64 Cal.2d 843.\n",
            "\n",
            "Can a defendant testify regarding a prior drunk driving conviction?\n",
            "\n",
            "Yes. A defendant has the general right to testify. People v. Flannel (1979), 25 Cal.3d 668.\n",
            "\n",
            "Can a defendant testify regarding an expert opinion?\n",
            "\n",
            "Yes. A defendant has the general right to testify. People v. Flannel (1979), 25 Cal.3d 668.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example of training fastText model and getting sentence embeddings"
      ],
      "metadata": {
        "id": "xl0K2GQZe8kb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCFRB215b45H"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText\n",
        "from scipy import spatial\n",
        "\n",
        "\n",
        "def get_sentence_embedding(model, sentence):\n",
        "  # This method takes in the trained model and the input sentence\n",
        "  # and returns the embedding of the sentence as the average embedding\n",
        "  # of its words\n",
        "  words = sentence.split(\" \")\n",
        "  vector = model.wv[words[0]]\n",
        "  for i in range(1, len(words)):\n",
        "    vector += model.wv[words[i]]\n",
        "  return vector/len(words)\n",
        "\n",
        "\n",
        "sampleTexts = [\"This is example1\", \"This is example two\", \"This is example three\"]\n",
        "# There are parameters here that you should define\n",
        "model = FastText(size = 100, window = 5, min_n=1)\n",
        "model.build_vocab(sampleTexts)\n",
        "\n",
        "# training the model\n",
        "model.train(sampleTexts, total_examples = len(sampleTexts), epochs = 10)\n",
        "\n",
        "# saving the model in-case you need to reuse it\n",
        "model.save(\"fastText.model\")\n",
        "\n",
        "vec1 = get_sentence_embedding(model, sampleTexts[0])\n",
        "vec2 = get_sentence_embedding(model, sampleTexts[1])\n",
        "vec3 = get_sentence_embedding(model, sampleTexts[2])\n",
        "\n",
        "# calculating cosine similarity\n",
        "result = 1 - spatial.distance.cosine(vec1, vec2)\n",
        "print(result)\n",
        "\n",
        "result = 1 - spatial.distance.cosine(vec1, vec3)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading Law Stack Exchange Data"
      ],
      "metadata": {
        "id": "1vC6-KZifEAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from post_parser_record import PostParserRecord\n",
        "\n",
        "\n",
        "def read_tsv_test_data(file_path):\n",
        "  # Takes in the file path for test file and generate a dictionary\n",
        "  # of question id as the key and the list of question ids similar to it\n",
        "  # as value. It also returns the list of all question ids that have\n",
        "  # at least one similar question\n",
        "  dic_similar_questions = {}\n",
        "  lst_all_test = []\n",
        "  with open(file_path) as fd:\n",
        "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
        "    for row in rd:\n",
        "        question_id = int(row[0])\n",
        "        lst_similar = list(map(int, row[1:]))\n",
        "        dic_similar_questions[question_id] = lst_similar\n",
        "        lst_all_test.append(question_id)\n",
        "        lst_all_test.extend(lst_similar)\n",
        "  return dic_similar_questions, lst_all_test\n",
        "\n",
        "\n",
        "def train_model(lst_sentences):\n",
        "  model = None\n",
        "  # training\n",
        "  # Your code\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def main():\n",
        "  duplicate_file = \"duplicate_questions.tsv\"\n",
        "  post_file = \"Posts_law.xml\"\n",
        "  dic_similar_questions, lst_all_test = read_tsv_test_data(duplicate_file)\n",
        "  post_reader = PostParserRecord(post_file)\n",
        "  lst_training_sentences = []\n",
        "  for question_id in post_reader.map_questions:\n",
        "    if question_id in lst_all_test:\n",
        "      continue\n",
        "    question = post_reader.map_questions[question_id]\n",
        "    title = question.title\n",
        "    body = question.body\n",
        "    # Collect sentences here\n",
        "\n",
        "    lst_answers = question.answers\n",
        "    if lst_answers is not None:\n",
        "      for answer in lst_answers:\n",
        "        answer_body = answer.body\n",
        "        # Collection sentences here\n",
        "        # Your code\n",
        "\n",
        "  # train your model\n",
        "  model = train_model(lst_training_sentences)\n",
        "\n",
        "  # This dictionary will have the test question id as the key\n",
        "  # and the most similar question id as the value\n",
        "  dictionary_result = {}\n",
        "\n",
        "  # finding Similar questions using fastText model\n",
        "  for test_question_id in dic_similar_questions:\n",
        "      \n",
        "      # for this question you have to find the similar questions\n",
        "      test_question = post_reader.map_questions[test_question_id]\n",
        "      test_title = question.title\n",
        "      test_body = question.body\n",
        "\n",
        "      for question_id in post_reader.map_questions:\n",
        "        # we are not comparing a question with itself\n",
        "        if question_id == test_question_id:\n",
        "          continue\n",
        "        test_question = post_reader.map_questions[test_question_id]\n",
        "        test_title = question.title\n",
        "        test_body = question.body\n",
        "\n",
        "          # use your model and calculate the cosine similarity between the questions\n",
        "          # save the question id with the highest cosine similarity\n",
        "\n",
        "          # Your code\n",
        "\n",
        "    # Calculate average P@1 and print it.\n",
        "    # Your code\n",
        "main()"
      ],
      "metadata": {
        "id": "M7PNV1oxfBbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}